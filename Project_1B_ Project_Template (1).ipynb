{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLEASE RUN THE FOLLOWING CODE FOR PRE-PROCESSING THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/workspace\n",
      "['/home/workspace/event_data/2018-11-18-events.csv', '/home/workspace/event_data/2018-11-02-events.csv', '/home/workspace/event_data/2018-11-03-events.csv', '/home/workspace/event_data/2018-11-13-events.csv', '/home/workspace/event_data/2018-11-14-events.csv', '/home/workspace/event_data/2018-11-10-events.csv', '/home/workspace/event_data/2018-11-22-events.csv', '/home/workspace/event_data/2018-11-30-events.csv', '/home/workspace/event_data/2018-11-17-events.csv', '/home/workspace/event_data/2018-11-05-events.csv', '/home/workspace/event_data/2018-11-23-events.csv', '/home/workspace/event_data/2018-11-12-events.csv', '/home/workspace/event_data/2018-11-29-events.csv', '/home/workspace/event_data/2018-11-08-events.csv', '/home/workspace/event_data/2018-11-19-events.csv', '/home/workspace/event_data/2018-11-28-events.csv', '/home/workspace/event_data/2018-11-26-events.csv', '/home/workspace/event_data/2018-11-04-events.csv', '/home/workspace/event_data/2018-11-16-events.csv', '/home/workspace/event_data/2018-11-24-events.csv', '/home/workspace/event_data/2018-11-15-events.csv', '/home/workspace/event_data/2018-11-01-events.csv', '/home/workspace/event_data/2018-11-07-events.csv', '/home/workspace/event_data/2018-11-21-events.csv', '/home/workspace/event_data/2018-11-11-events.csv', '/home/workspace/event_data/2018-11-06-events.csv', '/home/workspace/event_data/2018-11-25-events.csv', '/home/workspace/event_data/2018-11-09-events.csv', '/home/workspace/event_data/2018-11-20-events.csv', '/home/workspace/event_data/2018-11-27-events.csv']\n",
      "['/home/workspace/event_data/.ipynb_checkpoints/2018-11-05-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-07-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-09-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-12-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-30-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-14-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-04-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-01-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-02-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-13-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-03-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-08-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-06-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-11-events-checkpoint.csv', '/home/workspace/event_data/.ipynb_checkpoints/2018-11-10-events-checkpoint.csv']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print working directory '/home/workspace'\n",
    "print(os.getcwd())\n",
    "\n",
    "# the filepath is '/home/workspace/event_data'. Get all directories and sub-directories.\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "# The list of files and filepaths will be created in 'home/workspace/event_data'\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    \n",
    "## join the file path('home/workspace/event_data') and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "    # print the file_path_list\n",
    "    print(file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3429\n",
      "[['A Fine Frenzy', 'Logged In', 'Anabelle', 'F', '0', 'Simpson', '267.91138', 'free', 'Philadelphia-Camden-Wilmington, PA-NJ-DE-MD', 'PUT', 'NextSong', '1.54104E+12', '256', 'Almost Lover (Album Version)', '200', '1.54138E+12', '69'], ['Nirvana', 'Logged In', 'Aleena', 'F', '0', 'Kirby', '214.77832', 'paid', 'Waterloo-Cedar Falls, IA', 'PUT', 'NextSong', '1.54102E+12', '237', 'Serve The Servants', '200', '1.54138E+12', '44'], ['Television', 'Logged In', 'Aleena', 'F', '1', 'Kirby', '238.49751', 'paid', 'Waterloo-Cedar Falls, IA', 'PUT', 'NextSong', '1.54102E+12', '237', 'See No Evil  (Remastered LP Version)', '200', '1.54138E+12', '44'], ['JOHN COLTRANE', 'Logged In', 'Aleena', 'F', '2', 'Kirby', '346.43546', 'paid', 'Waterloo-Cedar Falls, IA', 'PUT', 'NextSong', '1.54102E+12', '237', 'Blues To Bechet (LP Version)', '200', '1.54138E+12', '44'], ['NOFX', 'Logged In', 'Aleena', 'F', '3', 'Kirby', '80.79628', 'paid', 'Waterloo-Cedar Falls, IA', 'PUT', 'NextSong', '1.54102E+12', '237', \"It's My Job To Keep Punk Rock Elite\", '200', '1.54138E+12', '44']]\n"
     ]
    }
   ],
   "source": [
    "# initialize the list array\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "## for every filepath in the file path list (all files in '/home/workspace/event_data')\n",
    "for f in file_path_list:\n",
    "\n",
    "## reading csv file (each file in 'home/workspace/event_data') \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        ## creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    "## extracting each data row one by one and append it and create full_data_rows_list file        \n",
    "        for line in csvreader:\n",
    "            ##print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "        #print(full_data_rows_list)\n",
    "# get total number of rows for full_data_rows_list \n",
    "print(len(full_data_rows_list))\n",
    "# print the first five rows in the list\n",
    "print(full_data_rows_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a smaller event data csv file called event_datafile_new csv that will be used to insert data into the\n",
    "# Apache Cassandra tables used for the three queries.\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "# create a new file event_datafile_new.csv\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    # label the columns in each row in event_datafile_new.csv  \n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    # for each row in the original list ,if first column is empty \n",
    "    # don't do anything ,just continue the loop until the first column has a value. If it has value then input\n",
    "    # columns 1,3,4,5,6,7,8,9,13,14,17 in event_data_new.csv. \n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2893\n"
     ]
    }
   ],
   "source": [
    "# count the number of rows in your event_datafile_new csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Fine Frenzy</td>\n",
       "      <td>Anabelle</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>Simpson</td>\n",
       "      <td>267.91138</td>\n",
       "      <td>free</td>\n",
       "      <td>Philadelphia-Camden-Wilmington, PA-NJ-DE-MD</td>\n",
       "      <td>256</td>\n",
       "      <td>Almost Lover (Album Version)</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nirvana</td>\n",
       "      <td>Aleena</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>Kirby</td>\n",
       "      <td>214.77832</td>\n",
       "      <td>paid</td>\n",
       "      <td>Waterloo-Cedar Falls, IA</td>\n",
       "      <td>237</td>\n",
       "      <td>Serve The Servants</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Television</td>\n",
       "      <td>Aleena</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>Kirby</td>\n",
       "      <td>238.49751</td>\n",
       "      <td>paid</td>\n",
       "      <td>Waterloo-Cedar Falls, IA</td>\n",
       "      <td>237</td>\n",
       "      <td>See No Evil  (Remastered LP Version)</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JOHN COLTRANE</td>\n",
       "      <td>Aleena</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>Kirby</td>\n",
       "      <td>346.43546</td>\n",
       "      <td>paid</td>\n",
       "      <td>Waterloo-Cedar Falls, IA</td>\n",
       "      <td>237</td>\n",
       "      <td>Blues To Bechet (LP Version)</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOFX</td>\n",
       "      <td>Aleena</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>Kirby</td>\n",
       "      <td>80.79628</td>\n",
       "      <td>paid</td>\n",
       "      <td>Waterloo-Cedar Falls, IA</td>\n",
       "      <td>237</td>\n",
       "      <td>It's My Job To Keep Punk Rock Elite</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          artist firstName gender  itemInSession lastName     length level  \\\n",
       "0  A Fine Frenzy  Anabelle      F              0  Simpson  267.91138  free   \n",
       "1        Nirvana    Aleena      F              0    Kirby  214.77832  paid   \n",
       "2     Television    Aleena      F              1    Kirby  238.49751  paid   \n",
       "3  JOHN COLTRANE    Aleena      F              2    Kirby  346.43546  paid   \n",
       "4           NOFX    Aleena      F              3    Kirby   80.79628  paid   \n",
       "\n",
       "                                      location  sessionId  \\\n",
       "0  Philadelphia-Camden-Wilmington, PA-NJ-DE-MD        256   \n",
       "1                     Waterloo-Cedar Falls, IA        237   \n",
       "2                     Waterloo-Cedar Falls, IA        237   \n",
       "3                     Waterloo-Cedar Falls, IA        237   \n",
       "4                     Waterloo-Cedar Falls, IA        237   \n",
       "\n",
       "                                   song  userId  \n",
       "0          Almost Lover (Album Version)      69  \n",
       "1                    Serve The Servants      44  \n",
       "2  See No Evil  (Remastered LP Version)      44  \n",
       "3          Blues To Bechet (LP Version)      44  \n",
       "4   It's My Job To Keep Punk Rock Elite      44  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 rows in event_datafile_new.csv in a dataframe format\n",
    "yourfile = pd.read_csv('event_datafile_new.csv',nrows=5)\n",
    "yourfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "# create a file 'event_datafile_new2.csv' where first name and last name are combined as username to input in\n",
    "# the Cassandra tables for the last two queries.\n",
    "with open('event_datafile_new2.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    # For new file 'event_datafile_new2.csv' label the columns \n",
    "    writer.writerow(['artist','username','gender','itemInSession','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    # for each row in the original list ,if first column is empty \n",
    "    # don't do anything ,just continue the loop until the first column has a value. If it has value then input\n",
    "    # in event_data_new2.csv, columns 1, the concatenation of the firstname and lastname columns as the username\n",
    "    # column and then the remaining columns ( 4,5,7,8,9,13,14,17)\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], (row[2]+' '+row[5]), row[3], row[4], row[6], row[7], row[8], row[12], row[13], row[16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>username</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Fine Frenzy</td>\n",
       "      <td>Anabelle Simpson</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>267.91138</td>\n",
       "      <td>free</td>\n",
       "      <td>Philadelphia-Camden-Wilmington, PA-NJ-DE-MD</td>\n",
       "      <td>256</td>\n",
       "      <td>Almost Lover (Album Version)</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nirvana</td>\n",
       "      <td>Aleena Kirby</td>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>214.77832</td>\n",
       "      <td>paid</td>\n",
       "      <td>Waterloo-Cedar Falls, IA</td>\n",
       "      <td>237</td>\n",
       "      <td>Serve The Servants</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Television</td>\n",
       "      <td>Aleena Kirby</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>238.49751</td>\n",
       "      <td>paid</td>\n",
       "      <td>Waterloo-Cedar Falls, IA</td>\n",
       "      <td>237</td>\n",
       "      <td>See No Evil  (Remastered LP Version)</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JOHN COLTRANE</td>\n",
       "      <td>Aleena Kirby</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>346.43546</td>\n",
       "      <td>paid</td>\n",
       "      <td>Waterloo-Cedar Falls, IA</td>\n",
       "      <td>237</td>\n",
       "      <td>Blues To Bechet (LP Version)</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOFX</td>\n",
       "      <td>Aleena Kirby</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>80.79628</td>\n",
       "      <td>paid</td>\n",
       "      <td>Waterloo-Cedar Falls, IA</td>\n",
       "      <td>237</td>\n",
       "      <td>It's My Job To Keep Punk Rock Elite</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          artist          username gender  itemInSession     length level  \\\n",
       "0  A Fine Frenzy  Anabelle Simpson      F              0  267.91138  free   \n",
       "1        Nirvana      Aleena Kirby      F              0  214.77832  paid   \n",
       "2     Television      Aleena Kirby      F              1  238.49751  paid   \n",
       "3  JOHN COLTRANE      Aleena Kirby      F              2  346.43546  paid   \n",
       "4           NOFX      Aleena Kirby      F              3   80.79628  paid   \n",
       "\n",
       "                                      location  sessionId  \\\n",
       "0  Philadelphia-Camden-Wilmington, PA-NJ-DE-MD        256   \n",
       "1                     Waterloo-Cedar Falls, IA        237   \n",
       "2                     Waterloo-Cedar Falls, IA        237   \n",
       "3                     Waterloo-Cedar Falls, IA        237   \n",
       "4                     Waterloo-Cedar Falls, IA        237   \n",
       "\n",
       "                                   song  userId  \n",
       "0          Almost Lover (Album Version)      69  \n",
       "1                    Serve The Servants      44  \n",
       "2  See No Evil  (Remastered LP Version)      44  \n",
       "3          Blues To Bechet (LP Version)      44  \n",
       "4   It's My Job To Keep Punk Rock Elite      44  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 rows in event_datafile_new2.csv in a dataframe format\n",
    "yourfile2 = pd.read_csv('event_datafile_new2.csv',nrows=5)\n",
    "yourfile2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Complete the Apache Cassandra coding portion of your project. \n",
    "\n",
    "## Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin writing your Apache Cassandra code in the cells below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should make a connection to a Cassandra instance your local machine \n",
    "# (127.0.0.1)\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster()\n",
    "\n",
    "# To establish connection and begin executing queries, need a session\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keyspace named keyspace_song. The keyspace will use one node for replication. \n",
    "# Class is simplestrategy as you are only running a one node cluster.\n",
    "try:\n",
    "    session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS keyspace_song \n",
    "    WITH REPLICATION = \n",
    "    { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }\"\"\"\n",
    ")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set KEYSPACE to keyspace_song specified above\n",
    "try:\n",
    "    session.set_keyspace('keyspace_song')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to create tables to run the following queries. Remember, with Apache Cassandra you model the database tables on the queries you want to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create queries to ask the following three questions of the data\n",
    "\n",
    "### 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "\n",
    "### 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "    \n",
    "\n",
    "### 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Query 1:\n",
    "# We need to find the artist, song title and song's length in the music app history list that was heard during\n",
    "# sessionId = 338, and itemInSession = 4. To do this we first create a table, song_length_session, \n",
    "# to contain all columns in table.\n",
    "# Since the query wants matches based on sessionId and itemInSession, the columns, sessionId and itemInSession\n",
    "# will be our primary key. sessionId is the partition key and itemInSession is the clustering column.\n",
    "# The partition key is used to determine what node gets the data.\n",
    "# The clustering column specifies the order that the data is arranged inside the partition.\n",
    "# Both sessionId and itemInSession are in the WHERE clause in the query.\n",
    "query1 = \"CREATE TABLE IF NOT EXISTS song_length_session \"\n",
    "query1 = query1 + \"(sessionId int, itemIdSession int, artist text, song text, length float, \\\n",
    "PRIMARY KEY (sessionId, itemIdSession))\" \n",
    "\n",
    "try:\n",
    "    session.execute(query1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The file , 'event_datafile_new.csv' will be used to enter values in the song_length_session table \n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "# open the 'event_datafile_new.csv' file\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "# The INSERT statement will insert sessionId, itemIdSession, artist, song and length into the `query1` variable \n",
    "# for the song_length_session table.\n",
    "        query1 = \"INSERT INTO song_length_session(sessionId, itemIdSession, artist, song, length)\"\n",
    "        query1 = query1 + \"VALUES (%s, %s, %s, %s, %s)\"\n",
    "        # The column elements from the music app history list are assigned for the sessionId, \n",
    "        # itemIdSession, artist, song, and length columns in the INSERT statement.\n",
    "        session.execute(query1, (int(line[8]), int(line[3]), str(line[0]), str(line[9]), float(line[5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a SELECT to verify that the data have been inserted into each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 0 Regina Spektor The Calculation (Album Version) 191.08526611328125\n",
      "23 1 Octopus Project All Of The Champs That Ever Lived 250.95791625976562\n",
      "23 2 Tegan And Sara So Jealous 180.06158447265625\n",
      "23 3 Dragonette Okay Dolores 153.39056396484375\n",
      "23 4 Lil Wayne / Eminem Drop The World 229.58975219726562\n"
     ]
    }
   ],
   "source": [
    "# The SELECT statement will verify that the data was entered into the song_length_session table\n",
    "query1 = \"SELECT * FROM song_length_session LIMIT 5\"\n",
    "try:\n",
    "    rows = session.execute(query1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.sessionid, row.itemidsession, row.artist, row.song, row.length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithless Music Matters (Mark Knight Dub) 495.30731201171875\n"
     ]
    }
   ],
   "source": [
    "# The following query will get the artist, song title and song's length in the music app history list that was heard during\n",
    "# sessionId = 338, and itemInSession = 4\n",
    "query1 = \"SELECT * FROM song_length_session WHERE sessionId = 338 AND itemIdSession = 4\"\n",
    "try:\n",
    "    rows = session.execute(query1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.artist, row.song, row.length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COPY AND REPEAT THE ABOVE THREE CELLS FOR EACH OF THE THREE QUESTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Query 2\n",
    "# We need to find name of artist, song(sorted by itemInSession) and user(first and last name)\n",
    "# for userid = 10, sessionid = 182. To do this we create a table, song_user_session, consisting of userId, sessionId,\n",
    "# itemInSession, artist, song and username(fistname and lastname). \n",
    "# Our partition key is userId and sessionId. ItemInSession is the clustering column sorted in ascending order. \n",
    "# The partition key specifies particular node and clustering column denotes sorting(in this case the default\n",
    "# which is ascending) within each partition. \n",
    "\n",
    "query2 = \"CREATE TABLE IF NOT EXISTS song_user_session \"\n",
    "query2 = query2 + \"(userId int, sessionId int, itemInSession int, artist text, song text, username text, \\\n",
    "PRIMARY KEY ((userId, sessionId), itemInSession)) WITH CLUSTERING ORDER BY (itemInSession ASC)\" \n",
    "\n",
    "try:\n",
    "    session.execute(query2)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the file 'event_datafile_new2.csv' is used for the song_user_session table\n",
    "file = 'event_datafile_new2.csv'\n",
    "\n",
    "# open the 'event_datafile_new2.csv' file\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "# The INSERT statement will insert userId, sessionId, itemInSession, artist, song, and username into \n",
    "# the `query2` variable for the song_user_session table. \n",
    "        query2 = \"INSERT INTO song_user_session(userId, sessionId, itemInSession, artist, song, username)\"\n",
    "        query2 = query2 + \"VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "        # The column elements from the music app history list are assigned for the userId, sessionId, \n",
    "        # itemIdSession, artist, song, and username columns in the INSERT statement.\n",
    "        session.execute(query2, (int(line[9]), int(line[7]), int(line[3]), str(line[0]), str(line[8]), str(line[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 1068 0 1 Mile North Black Lines Ryan Smith\n",
      "26 1068 1 USS (Ubiquitous Synergy Seeker) Man Makes The Zoo Ryan Smith\n",
      "26 1068 2 EsmÃÂ©e Denters / Justin Timberlake Love Dealer (Featuring Justin Timberlake) Ryan Smith\n",
      "26 1068 3 Train Hey_ Soul Sister Ryan Smith\n",
      "26 1068 6 The Pussycat Dolls / Snoop Dogg Bottle Pop Ryan Smith\n"
     ]
    }
   ],
   "source": [
    "# The SELECT statement is used to verify the data was entered into the song_user_session table\n",
    "query2 = \"SELECT * FROM song_user_session LIMIT 5\"\n",
    "try:\n",
    "    rows = session.execute(query2)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.userid, row.sessionid, row.iteminsession, row.artist, row.song, row.username)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Down To The Bone Keep On Keepin' On Sylvie Cruz\n",
      "Three Drives Greece 2000 Sylvie Cruz\n",
      "Sebastien Tellier Kilometer Sylvie Cruz\n",
      "Lonnie Gordon Catch You Baby (Steve Pitron & Max Sanna Radio Edit) Sylvie Cruz\n"
     ]
    }
   ],
   "source": [
    "# Use the SELECT statement to get the name of the artist, song(sorted by itemInSession) and user(first and last name)\n",
    "# for userid = 10, sessionid = 182 from the song_user_session table. \n",
    "query2 = \"SELECT * FROM song_user_session WHERE userId = 10 AND sessionId = 182\" \n",
    "\n",
    "try:\n",
    "    rows = session.execute(query2)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.artist, row.song, row.username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Query 3\n",
    "# We need to find every user name (first and last) in the music app history list who listened to the song\n",
    "# 'All Hands Against His Own' \n",
    "# To do this we create table user_song_allHands with userid , song and username. \n",
    "# We use compound primary key of song and userId. Song is our partition key and userId is our\n",
    "# clustering key. Since song is unique in this query it will be the partition key which will go\n",
    "# on one node. For a quicker read you would want to put song on a single node and search only that node.\n",
    "# The clustering column will be userId since it is an integer and specifies the the order that the \n",
    "# data is arranged inside the single partition.\n",
    "query3 = \"CREATE TABLE IF NOT EXISTS user_song_allHands \"\n",
    "query3 = query3 + \"(userId int, song text, username text, PRIMARY KEY(song,userId))\"\n",
    "\n",
    "\n",
    "try:\n",
    "    session.execute(query3)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the file 'event_datafile_new2.csv' is used for the user_song_allHands table\n",
    "file = 'event_datafile_new2.csv'\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader:\n",
    "# The INSERT statement will insert userId, song, and username into the `query3` variable \n",
    "# for the user_song_allHands table.\n",
    "        query3 = \"INSERT INTO user_song_allHands(userId, song, username)\"\n",
    "        query3 = query3 + \"VALUES (%s, %s, %s)\"\n",
    "        # The column elements from the music app history list are assigned for the userId, song \n",
    "        # and username columns in the INSERT statement. \n",
    "        session.execute(query3, (int(line[9]),str(line[8]),str(line[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 Too Tough (1994 Digital Remaster) Aleena Kirby\n",
      "73 My Place Jacob Klein\n",
      "25 Misfit Love Jayden Graves\n",
      "95 Eat To Live (Amended Version) Sara Johnson\n",
      "9 Hey_ Soul Sister Wyatt Scott\n"
     ]
    }
   ],
   "source": [
    "# The SELECT statement is used to verify that data was entered into the table user_song_allHands table\n",
    "query3 = \"SELECT * FROM user_song_allHands LIMIT 5\"\n",
    "try:\n",
    "    rows = session.execute(query3)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.userid, row.song, row.username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacqueline Lynch\n",
      "Sara Johnson\n"
     ]
    }
   ],
   "source": [
    "# The query will give every user name (first and last) in the music app history who listened to the song\n",
    "# 'All Hands Against His Own'\n",
    "query3 = \"SELECT * FROM user_song_allHands WHERE song = 'All Hands Against His Own'\"\n",
    "try:\n",
    "    rows = session.execute(query3)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print (row.username)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the song_length_session table\n",
    "query = \"drop table song_length_session\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the song_user_session table\n",
    "query = \"drop table song_user_session\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the user_song_allHands table\n",
    "query = \"drop table user_song_allHands\"\n",
    "try:\n",
    "    rows = session.execute(query)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
